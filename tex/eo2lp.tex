\documentclass{article}

\usepackage{ebproof}
\usepackage{array}
\usepackage{unicode-math}
\usepackage{fontspec}
\usepackage{xcolor}
\usepackage{xcolor-material}
\usepackage{hyperref}

\usepackage{newcomputermodern}

\usepackage{jbw-boxfigure}
\usepackage{jbw-fix-hyperref-autoref}

% \setmathfont{NewCMMath-Regular.otf}

% custom environment for subfigures
\newenvironment{cmdsubfig}[2]{%
    \begin{subfigure}[t]{\textwidth}
      \subcaption{\label{#1}#2}
      \captionsetup{subrefformat=parens}
      \centering
      \vspace{1mm}
}{%
      \vspace{1mm}
    \end{subfigure}
}

% font faces
\newcommand{\msf}{\mathsf}
\newcommand{\mbf}{\mathbf}
\newcommand{\mscr}{\mathscr}
\newcommand{\mtt}[1]{{\color{MaterialPink500}{\texttt{#1}}}}
\newcommand{\mcomment}[1]{\color{MaterialIndigo500}{\text{#1}}}
\newcommand{\parens}[1]{{\color{MaterialGrey600}{\lparen}}{#1}{\color{MaterialGrey600}{\rparen}}}

% ----- abstract syntax ----------
% plurality, binding
\newcommand{\plur}[2]{{#1}_{1}, \ldots, {#1}_{#2}}
\newcommand{\binddot}[3]{{#1\,#2.\,#3}}
% variables
\newcommand{\var}[1]{\msf{v}_{\msf{#1}}}
% binders
\newcommand{\lam}{\binddot{\msf{λ}}}
\newcommand{\pii}{\binddot{\msf{Π}}}
\newcommand{\lett}[2]{\msf{let}\ #1\,\msf{in}\ #2}
% applications
\newcommand{\appSym}{\cdot}
\newcommand{\app}[2]{{#1}\appSym{#2}}
\newcommand{\apptwo}[3]{#1\,#2\,#3}
\newcommand{\appthree}[4]{#1\,#2\,#3\,#4}
\newcommand{\appfour}[5]{#1\,#2\,#3\,#4\,#5}
\newcommand{\appldots}[3]{((\app{#1}{#2})\,\ldots\,{#3})}
% universes
\newcommand{\PROP}{\mtt{prop}}
\newcommand{\TYPE}{\mtt{type}}
\newcommand{\KIND}{\mtt{kind}}
% sets for syntactic categories
\newcommand{\Term}{\msf{Term}}

\title{
  Automatically Translating Proof Systems
  for SMT Solvers to the $\lambda\Pi$-calculus}
\author{Ciarán Dunne, Guillaume Burel}

\usepackage[backend=biber]{biblatex}
\addbibresource{inria.bib}

\begin{document}
\maketitle
% we're not just translating proofs, but also the proof systems embedded in Eunoia.

\begin{abstract}
	\noindent
	Eunoia is a logical framework used for formally specifying the
	theories, inference rules, and derivations of SMT solvers with
	proof production facilities.
	%
	We present a tool \texttt{eo2lp}, for automatically translating Eunoia
	specifications and proofs into code for \texttt{lambdapi} ---
	an interactive theorem prover based on the $\lambda\Pi$-calculus.
	%
	Our approach is demonstrated by
	(a) translating the portion of \texttt{cvc5}'s
	\textit{co-operating proof calculus} (CPC) corresponding to the QF-UF
	fragment of SMT-LIB; and
	%
	(b) translating proofs produced by running \texttt{cvc5} on a set of
	QF-UF problems from the SMT-LIB benchmark library.
\end{abstract}

\section{Background}

The area of automated reasoning known as \emph{satisfiability modulo theories}
(SMT) is aimed primarily at developing tools for deciding the (un)satisfiability
of logical specifications from various mathematical theories~\cite{Barrett2021}.
%
When a specification is deemed to be unsatisfiable, many solvers are now capable
of producing proofs that demonstrate this fact.
%
Many proof formats exist (e.g., Alethe, LFSC), but the inference rules used
in their proofs are not specified in any formal language.
%
This can make the development of proof checkers challenging, as developers
must rewrite parts of their checker for every change made to the proof system.

\textit{Eunoia} is a logical framework that allows formalizing the inference
rules used by the proof production facilities of an SMT solver.
%
\textit{Ethos} is a C++ tool for verifying proofs in the Eunoia format.

% eunoia is similar/inspired by the speculative proposal for SMT-LIB 3.
SMT-LIB is a specification language used for interacting with SMT solvers.
The syntax and semantics of SMT-LIB are standardized by the
SMT-LIB Initiative~\autocite{Barrett2015-standard}).
%
Generally speaking, SMT-LIB scripts specify satisfiability problems for
many-sorted first-order logic (MFOL) and provide instructions for solvers
to interact with these specifications.


\section{Eunoia}

% we won't bother showing the formal details of the datatype used for eunoia
% concrete syntax. we'll just write the concrete syntax with metavariables
% inside.. we will justify this treatment by explaining our parser, lexer.
%
% but we will still develop an abstract notion of 'eunoia signature' etc.
The overall architecture of our method is as follows:
\begin{itemize}
	\item We parse Eunoia files using an \texttt{ocamllex} lexer and a parser
	      generated by \texttt{menhir}. The grammar used for generating the
	      parser is largely based off the grammar in the Eunoia user manual.

	\item We translate Eunoia specifications to an abstract syntax given by an
	      OCaml datatype. During this phase, we eliminate applications of
	      operators declared to be $n$-ary (e.g., \texttt{and}, \texttt{or}).
\end{itemize}

\section{Interpreting Eunoia in the λΠ-calculus}

\subsection{Terms, Judgements, and Inferences}
%

% TODO. fix \typecolon. seperate grammar for judgements, rewrite rules, signatures, ...
\begin{boxfigure}[t!]{fig:term-syntax}
	{
		Syntax for terms, contexts, judgements of $λΠ^\ast$.
	}
	$$
		\begin{array}
			{r@{\ ⦂ \ }l@{\quad}l@{\qquad}r@{\ ⦂ \ }l@{\qquad}r@{}}
			%
			x,y,z & \var 0 ∣ \var 1 ∣ \var 2 ∣ \ldots
			      & \mcomment{(variables)}
			      &
			%
			u     & \TYPE ∣ \PROP ∣ \KIND
			      & \mcomment{(universes)}
			\\
			%
			e, t  &
			\multicolumn{4}{@{}l@{}}{
			x ∣ u ∣ \parens{\app{t_1}{t_2}}
			∣ \parens{\lam{x : t_1}{t_2}}
			∣ \parens{\pii{x : t_1}{t_2}}
			∣ \parens{\lett{\parens{x ≔ t_1}}{t_2}}
			}
			      & \mcomment{(terms)}
		\end{array}
	$$
	$$
		\begin{array}{r@{\ ⦂ \ }l@{\quad}l}
			γ & \parens{t_1 ∷ t_2} \mid \parens{\parens{t_1 ≡ t_2} ∷ t_3}
			  & \mcomment{(context elements)}
			\\
			r & \parens{t_1 ↪ t_2}
			  & \mcomment{(rewrite clause)}
			\\
			κ &
			\parens{\msf{decl}\ x ∷ t} \mid
			\parens{\msf{defn}\ x ∷ t_1 ≔ t_2} \mid
			\parens{\msf{rule}\ {r_1{\ldots}r_n}}
			  &
			\mcomment{(theory elements)}
		\end{array}
	$$
\end{boxfigure}
%
Let $\Term$ be the set of \emph{terms} given by the language generated by the
non-terminal $t$ with the production rules given in \autoref{fig:term-syntax}.
%
Each term is either a \emph{variable},
a \emph{universe} from the set $\{ \TYPE, \PROP, \KIND \}$,
an \emph{application} of two terms $(\app{t_1}{t_2})$,
a \emph{abstraction} $(\binddot{\mscr B} {x : t_1} {t_2})$,
where $\mscr{B}$ is a \emph{binder} from the set of symbols $\{ λ, Π \}$,
or a \emph{local definition} $(\lett{x ≡ t_1}{t_2})$.
%
A \emph{context element} is either a \emph{typing} $(t_1 ∷ t_2)$,
or a \emph{typed equality} $(t_1 ≡ t_2) ∷ t_3$.
%
A \emph{context} is a list $Γ$ of context elements.
%

A \emph{theory element} is either a \emph{declaration}, a \emph{definition},
or a \emph{rewrite rule}.

\printbibliography

\end{document}
